{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V71pFDP1-mjz"
   },
   "source": [
    "**Getting Started with Natural Language Processing.**\n",
    "\n",
    "Let's get started with a sample project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzebX0hx-83V"
   },
   "source": [
    "Start with importing the nltk library. To make sure you have all the packages, run nltk.download() and select all in download option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTOqHsrzz8fQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 667,
     "status": "ok",
     "timestamp": 1615860952571,
     "user": {
      "displayName": "Jyoti Gupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gha7ARiuH9KYsBrRe_sEPEhpI5cXr55TdQBv6x_MA=s64",
      "userId": "16785426337890807929"
     },
     "user_tz": -480
    },
    "id": "eyEj-cyqz9TX",
    "outputId": "c584c6a1-0270-4abe-c835-af8da750cc9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive/FTDS Student Material/04MLApplications/03NLP/01NLPIntro_Auto-Summarization')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "id": "FGYz1x73_lzt"
   },
   "outputs": [],
   "source": [
    "#!pip3 install nltk\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z_LZ_5-daZk4",
    "outputId": "4ca99621-a7eb-4390-f008-a0dcddb3126d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()\n",
    "#It takes a long time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWtOZZV8_rd2"
   },
   "source": [
    "## Tokenize\n",
    "Let's start with 2 random lines of text. \n",
    "\n",
    "**Q1: Start with tokenizing it into sentence using the package.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "id": "Q-3CN_7SAmAR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I live in Hong Kong.', 'Hong Kong is an amazing city']\n"
     ]
    }
   ],
   "source": [
    "text=\"I live in Hong Kong. Hong Kong is an amazing city\"\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "#To segment the text into sentences, use sent_tokenize\n",
    "# sent_tokenize: seperate into sentences\n",
    "sents=sent_tokenize(text)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "executionInfo": {
     "elapsed": 1818,
     "status": "ok",
     "timestamp": 1591931058398,
     "user": {
      "displayName": "Maggie Chuang",
      "photoUrl": "",
      "userId": "14813054528533700671"
     },
     "user_tz": -480
    },
    "id": "t87pC_Q8fB9v",
    "outputId": "4cfde828-c9f3-4787-9d39-75006767d3d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6_ovBHiAw33"
   },
   "source": [
    "**Q2: Print the words out of the last tokenized line**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "r1rEAK1aA5z1",
    "outputId": "4f8af60e-79de-474a-a49a-7423fda50ba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'live', 'in', 'Hong', 'Kong', '.'], ['Hong', 'Kong', 'is', 'an', 'amazing', 'city']]\n"
     ]
    }
   ],
   "source": [
    "#word_tokenize is used to split the sentences into words\n",
    "words=[word_tokenize(sent) for sent in sents]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3oqQslU7b9lQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_btj55KgBAG5"
   },
   "source": [
    "**Q3: Now it's time to remove stopwords and punctuation. Start with calling the package first.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "id": "sMajTyVDBFaV",
    "outputId": "6b432620-031d-475f-86a0-914bb65616df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "# stop words are words which are filtered out before or after processing of natural language data\n",
    "# We would not want these words taking up space in our database, or taking up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to be stop words. \n",
    "from nltk.corpus import stopwords \n",
    "from string import punctuation\n",
    "customStopWords=stopwords.words('english')+list(punctuation)\n",
    "print(customStopWords)\n",
    "# can custom Stop Words by also input own stopword manually;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgmXws8qBMr_"
   },
   "source": [
    "**Q4: Now print out the tokenized words without punctuations and stopwords.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "d2OD1Ifdb9lX",
    "outputId": "20dcb7ca-0b53-492f-e4cb-4524a172b2dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'live', 'Hong', 'Kong', 'Hong', 'Kong', 'amazing', 'city']\n"
     ]
    }
   ],
   "source": [
    "wordsWOStopwords=[word for word in word_tokenize(text) if word not in customStopWords]\n",
    "print(wordsWOStopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Id5mwmHBqjB"
   },
   "source": [
    "## Ngram\n",
    "**Q5: Now it's time to use Bigram Association measures and print the ngram items.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "F42kMbuCB1Ye",
    "outputId": "d20f7970-6e57-488f-c5fc-92649096f192"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([(('I', 'live'), 1), (('live', 'Hong'), 1), (('Hong', 'Kong'), 2), (('Kong', 'Hong'), 1), (('Kong', 'amazing'), 1), (('amazing', 'city'), 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('Hong', 'Kong'), 2),\n",
       " (('I', 'live'), 1),\n",
       " (('Kong', 'Hong'), 1),\n",
       " (('Kong', 'amazing'), 1),\n",
       " (('amazing', 'city'), 1),\n",
       " (('live', 'Hong'), 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing pairs of words to see which pair is \"more likely to occur\" in US English than another pair. \n",
    "# Use the collocation facilities in NLTK to score word pairs, the higher scoring pair being the most likely.\n",
    "from nltk.collocations import *\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(wordsWOStopwords)\n",
    "\n",
    "#fequency of each pair\n",
    "print(finder.ngram_fd.items())\n",
    "\n",
    "sorted(finder.ngram_fd.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4OU9dxJB9QO"
   },
   "source": [
    "## Stemming OR Lemmatization\n",
    "\n",
    "**Q6: Now take another line of next and print the stemmed words using LancasterStemmer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "49ExxPWnCEO_",
    "outputId": "cd25c252-b974-4b8d-b67e-c127e1b548a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['our', 'feet', 'ar', 'wet', '.', 'my', 'foot', 'is', 'wet', '.']\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "# stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form\n",
    "text2 = \"Our feet are wet. My foot is wet.  \"\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "st=LancasterStemmer()\n",
    "stemmedWords=[st.stem(word) for word in word_tokenize(text2)]\n",
    "print(stemmedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "EK09WpLoZ2Df",
    "outputId": "0ceac216-b840-4ea8-ae5c-e1083036336c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Our', 'foot', 'are', 'wet', '.', 'My', 'foot', 'is', 'wet', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization is the process of converting a word to its base form. \n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "text2 = \"Our feet are wet. My foot is wet.  \"\n",
    "\n",
    "# Wordnet is an large, freely and publicly available lexical database for the English language\n",
    "# aim is to establish structured semantic relationships between words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print([lemmatizer.lemmatize(word) for word in word_tokenize(text2)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5m2UIp8CMsQ"
   },
   "source": [
    "**Q7: Use pos tag and tokenize the words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "id": "uOu4HVXZCbOw",
    "outputId": "9f60e5bc-9e6a-4865-9e0a-d31efad49d6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Our', 'PRP$'),\n",
       " ('feet', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('wet', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('My', 'PRP$'),\n",
       " ('foot', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('wet', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pos is part of speech. \n",
    "# nltk.pos_tag() returns a tuple with the POS tag. The key here is to map NLTK’s POS tags to the format wordnet lemmatizer would accept.\n",
    "nltk.pos_tag(word_tokenize(text2))\n",
    "\n",
    "# eg. JJ -Adjective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yb7h75_DBI6"
   },
   "source": [
    "**Q8: Now import wordnet and print out the sysnset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "id": "5kZ9rjXhDO05",
    "outputId": "6dc7d4bc-6751-4f59-ccae-13d5ac2bec42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bass.n.01') the lowest part of the musical range\n",
      "Synset('bass.n.02') the lowest part in polyphonic music\n",
      "Synset('bass.n.03') an adult male singer with the lowest voice\n",
      "Synset('sea_bass.n.01') the lean flesh of a saltwater fish of the family Serranidae\n",
      "Synset('freshwater_bass.n.01') any of various North American freshwater fish with lean flesh (especially of the genus Micropterus)\n",
      "Synset('bass.n.06') the lowest adult male singing voice\n",
      "Synset('bass.n.07') the member with the lowest range of a family of musical instruments\n",
      "Synset('bass.n.08') nontechnical name for any of numerous edible marine and freshwater spiny-finned fishes\n",
      "Synset('bass.s.01') having or denoting a low vocal or instrumental range\n"
     ]
    }
   ],
   "source": [
    "# WordNet is a lexical database for the English language.\n",
    "# It groups English words into sets of synonyms called synsets, provides short definitions and usage examples, \n",
    "# and records a number of relations among these synonym sets or their members. \n",
    "\n",
    "# like dictionary\n",
    "\n",
    "for ss in wn.synsets('bass'):\n",
    "    print(ss, ss.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sY27zIMQb9l4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYyDCvxFDS4F"
   },
   "source": [
    "**Q9: It's time to play around with lesk now. Import that and tokenize a new line to find the Synset.**\n",
    "\n",
    "*Suggested: Sing in a lower tone, along with the bass*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "id": "InABhWXQECns",
    "outputId": "9c03426f-34d1-499f-9b0f-cd227df27aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bass.n.07') the member with the lowest range of a family of musical instruments\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "# a classical algorithm for word sense *disambiguation* (Another library)\n",
    "# The Lesk algorithm is based on the assumption that words in a given \"neighborhood\" (section of text) will tend to share a common topic.\n",
    "sense1 = lesk(word_tokenize(\"Sing in a lower tone, along with the bass\"),'bass')\n",
    "print(sense1, sense1.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpzzXdTdEPEJ"
   },
   "source": [
    "**Q10: Try this another way.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "executionInfo": {
     "elapsed": 1582,
     "status": "error",
     "timestamp": 1591932170541,
     "user": {
      "displayName": "Maggie Chuang",
      "photoUrl": "",
      "userId": "14813054528533700671"
     },
     "user_tz": -480
    },
    "id": "K8yB1LwqEmIh",
    "outputId": "86483299-5ac8-4738-846d-066870db46b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('sea_bass.n.01') the lean flesh of a saltwater fish of the family Serranidae\n"
     ]
    }
   ],
   "source": [
    "sense2 = lesk(word_tokenize(\"This sea bass was really hard to catch\"),'bass')\n",
    "print(sense2, sense2.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2D9pcuZ7rZp"
   },
   "source": [
    "**I like to code. The code should be clean. What is the code of conduct? **\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hE4kSKx-Z2EX"
   },
   "outputs": [],
   "source": [
    "#This blew my mind in high school, and I wasn’t the only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 788,
     "status": "ok",
     "timestamp": 1591932357758,
     "user": {
      "displayName": "Maggie Chuang",
      "photoUrl": "",
      "userId": "14813054528533700671"
     },
     "user_tz": -480
    },
    "id": "FQnQX-s_jUuJ",
    "outputId": "8c369bc6-7075-4bb1-ff21-fffc5356c019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('rose.s.01') of something having a dusty purplish pink color\n"
     ]
    }
   ],
   "source": [
    " from nltk.wsd import lesk\n",
    "\n",
    "sense2 = lesk(word_tokenize(\"Because a rose is a rose is a rose is a rose.\"),'rose')\n",
    "print(sense2, sense2.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('planning', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('visit', 'VB'),\n",
       " ('Wan', 'NNP'),\n",
       " ('Chai', 'NNP'),\n",
       " ('to', 'TO'),\n",
       " ('attend', 'VB'),\n",
       " ('Tech', 'NNP'),\n",
       " ('Wan', 'NNP'),\n",
       " ('Chai', 'NNP'),\n",
       " ('Hackathon', 'NNP')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "text1 = 'I am planning to visit Wan Chai to attend Tech Wan Chai Hackathon'\n",
    "nltk.pos_tag(word_tokenize(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "01Concepts_NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
